{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# RAG Evaluation Analysis for Research Paper\n",
        "\n",
        "This notebook provides comprehensive analysis and visualization of RAG evaluation results for academic research.\n",
        "\n",
        "## Table of Contents\n",
        "1. Load and explore evaluation results\n",
        "2. Statistical analysis\n",
        "3. Performance comparisons\n",
        "4. Visualization for paper\n",
        "5. Statistical significance testing\n",
        "6. Export tables for LaTeX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style for academic paper quality plots\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure pandas display\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 3)\n",
        "\n",
        "print(\"üìä RAG Evaluation Analysis Notebook\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Evaluation Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the most recent evaluation results\n",
        "def load_latest_results(pattern=\"evaluation_results_*\"):\n",
        "    \"\"\"Load the most recent evaluation results\"\"\"\n",
        "    result_dirs = glob.glob(pattern)\n",
        "    if not result_dirs:\n",
        "        print(\"‚ùå No evaluation results found!\")\n",
        "        return None, None\n",
        "    \n",
        "    # Get most recent directory\n",
        "    latest_dir = max(result_dirs, key=os.path.getmtime)\n",
        "    print(f\"üìÅ Loading results from: {latest_dir}\")\n",
        "    \n",
        "    # Load CSV results\n",
        "    results_df = pd.read_csv(f\"{latest_dir}/full_results.csv\")\n",
        "    \n",
        "    # Load JSON analysis report\n",
        "    with open(f\"{latest_dir}/analysis_report.json\", 'r') as f:\n",
        "        analysis_report = json.load(f)\n",
        "    \n",
        "    return results_df, analysis_report\n",
        "\n",
        "# Load results\n",
        "results_df, analysis_report = load_latest_results()\n",
        "\n",
        "if results_df is not None:\n",
        "    print(f\"\\n‚úÖ Loaded {len(results_df)} evaluation results\")\n",
        "    print(f\"üìä Approaches evaluated: {results_df['approach'].unique()}\")\n",
        "    print(f\"‚ùì Questions: {results_df['question'].nunique()}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(\"\\nüìà Columns in dataset:\")\n",
        "    print(results_df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Summary Statistics and Performance Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary statistics\n",
        "if results_df is not None:\n",
        "    # Define metrics to analyze\n",
        "    quality_metrics = ['answer_relevancy', 'faithfulness', 'context_recall', \n",
        "                       'context_precision', 'answer_correctness']\n",
        "    performance_metrics = ['latency_ms', 'retrieval_time_ms', 'generation_time_ms']\n",
        "    \n",
        "    # Create summary table\n",
        "    summary_stats = results_df.groupby('approach')[quality_metrics + performance_metrics].agg(['mean', 'std'])\n",
        "    \n",
        "    # Round for display\n",
        "    summary_stats = summary_stats.round(3)\n",
        "    \n",
        "    print(\"üìä SUMMARY STATISTICS BY APPROACH\")\n",
        "    print(\"=\" * 80)\n",
        "    display(summary_stats)\n",
        "    \n",
        "    # Best performing approach for each metric\n",
        "    print(\"\\nüèÜ BEST PERFORMING APPROACHES\")\n",
        "    print(\"-\" * 50)\n",
        "    for metric in quality_metrics:\n",
        "        best_approach = results_df.groupby('approach')[metric].mean().idxmax()\n",
        "        best_score = results_df.groupby('approach')[metric].mean().max()\n",
        "        print(f\"{metric:20s}: {best_approach:20s} (score: {best_score:.3f})\")\n",
        "    \n",
        "    print(\"\\n‚ö° FASTEST APPROACHES\")\n",
        "    print(\"-\" * 50)\n",
        "    fastest = results_df.groupby('approach')['latency_ms'].mean().sort_values()\n",
        "    for approach, latency in fastest.head(3).items():\n",
        "        print(f\"{approach:20s}: {latency:.1f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Performance Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations for paper\n",
        "if results_df is not None:\n",
        "    # 1. Overall Quality Metrics Comparison\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for idx, metric in enumerate(quality_metrics):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Create box plot\n",
        "        data_to_plot = [results_df[results_df['approach'] == approach][metric].dropna() \n",
        "                        for approach in results_df['approach'].unique()]\n",
        "        \n",
        "        bp = ax.boxplot(data_to_plot, patch_artist=True, showmeans=True)\n",
        "        \n",
        "        # Customize box plot\n",
        "        for patch, color in zip(bp['boxes'], sns.color_palette(\"husl\", len(data_to_plot))):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_alpha(0.7)\n",
        "        \n",
        "        ax.set_xticklabels(results_df['approach'].unique(), rotation=45, ha='right')\n",
        "        ax.set_title(metric.replace('_', ' ').title(), fontsize=12)\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Remove empty subplot if any\n",
        "    if len(quality_metrics) < 6:\n",
        "        fig.delaxes(axes[-1])\n",
        "    \n",
        "    plt.suptitle('Quality Metrics Distribution by RAG Approach', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('quality_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Performance vs Quality Trade-off\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    # Calculate mean scores for plotting\n",
        "    approach_means = results_df.groupby('approach').agg({\n",
        "        'answer_correctness': 'mean',\n",
        "        'latency_ms': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(approach_means['latency_ms'], \n",
        "                        approach_means['answer_correctness'],\n",
        "                        s=200, alpha=0.7)\n",
        "    \n",
        "    # Add labels\n",
        "    for idx, row in approach_means.iterrows():\n",
        "        ax.annotate(row['approach'], \n",
        "                   (row['latency_ms'], row['answer_correctness']),\n",
        "                   xytext=(5, 5), textcoords='offset points',\n",
        "                   fontsize=10)\n",
        "    \n",
        "    ax.set_xlabel('Average Latency (ms)', fontsize=12)\n",
        "    ax.set_ylabel('Average Answer Correctness', fontsize=12)\n",
        "    ax.set_title('Performance vs Quality Trade-off', fontsize=14)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add ideal region\n",
        "    ax.axhspan(0.8, 1.0, alpha=0.1, color='green', label='High Quality')\n",
        "    ax.axvspan(0, 200, alpha=0.1, color='blue', label='Low Latency')\n",
        "    ax.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_quality_tradeoff.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Statistical Significance Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical significance testing between approaches\n",
        "if results_df is not None:\n",
        "    from scipy.stats import friedmanchisquare, wilcoxon\n",
        "    import itertools\n",
        "    \n",
        "    print(\"üìä STATISTICAL SIGNIFICANCE TESTING\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Prepare data for Friedman test (non-parametric for related samples)\n",
        "    approaches = results_df['approach'].unique()\n",
        "    \n",
        "    # Test each quality metric\n",
        "    for metric in quality_metrics:\n",
        "        print(f\"\\nüìà Testing metric: {metric}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Prepare data for each approach\n",
        "        approach_data = []\n",
        "        valid_approaches = []\n",
        "        \n",
        "        for approach in approaches:\n",
        "            data = results_df[results_df['approach'] == approach][metric].dropna()\n",
        "            if len(data) > 0:\n",
        "                approach_data.append(data.values)\n",
        "                valid_approaches.append(approach)\n",
        "        \n",
        "        if len(approach_data) >= 3:\n",
        "            # Friedman test\n",
        "            stat, p_value = friedmanchisquare(*approach_data)\n",
        "            print(f\"Friedman test: œá¬≤ = {stat:.3f}, p = {p_value:.4f}\")\n",
        "            \n",
        "            if p_value < 0.05:\n",
        "                print(\"‚úÖ Significant differences found! Performing pairwise comparisons...\")\n",
        "                \n",
        "                # Pairwise Wilcoxon signed-rank tests with Bonferroni correction\n",
        "                n_comparisons = len(list(itertools.combinations(valid_approaches, 2)))\n",
        "                bonferroni_alpha = 0.05 / n_comparisons\n",
        "                \n",
        "                print(f\"\\nPairwise comparisons (Bonferroni corrected Œ± = {bonferroni_alpha:.4f}):\")\n",
        "                \n",
        "                for i, j in itertools.combinations(range(len(valid_approaches)), 2):\n",
        "                    approach1, approach2 = valid_approaches[i], valid_approaches[j]\n",
        "                    data1 = results_df[results_df['approach'] == approach1][metric].dropna()\n",
        "                    data2 = results_df[results_df['approach'] == approach2][metric].dropna()\n",
        "                    \n",
        "                    # Align data by question for paired test\n",
        "                    questions = set(results_df[results_df['approach'] == approach1]['question']) & \\\n",
        "                                set(results_df[results_df['approach'] == approach2]['question'])\n",
        "                    \n",
        "                    aligned_data1 = []\n",
        "                    aligned_data2 = []\n",
        "                    \n",
        "                    for q in questions:\n",
        "                        val1 = results_df[(results_df['approach'] == approach1) & \n",
        "                                         (results_df['question'] == q)][metric].values\n",
        "                        val2 = results_df[(results_df['approach'] == approach2) & \n",
        "                                         (results_df['question'] == q)][metric].values\n",
        "                        \n",
        "                        if len(val1) > 0 and len(val2) > 0:\n",
        "                            aligned_data1.append(val1[0])\n",
        "                            aligned_data2.append(val2[0])\n",
        "                    \n",
        "                    if len(aligned_data1) > 5:  # Need sufficient samples\n",
        "                        stat, p = wilcoxon(aligned_data1, aligned_data2)\n",
        "                        sig = \"***\" if p < bonferroni_alpha else \"\"\n",
        "                        mean_diff = np.mean(aligned_data1) - np.mean(aligned_data2)\n",
        "                        print(f\"  {approach1} vs {approach2}: p={p:.4f} {sig}, mean_diff={mean_diff:.3f}\")\n",
        "            else:\n",
        "                print(\"‚ùå No significant differences found.\")\n",
        "                \n",
        "    # Create significance matrix for answer_correctness\n",
        "    print(\"\\nüìä SIGNIFICANCE MATRIX FOR ANSWER CORRECTNESS\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    sig_matrix = pd.DataFrame(index=approaches, columns=approaches, data=\"-\")\n",
        "    \n",
        "    for i, j in itertools.combinations(range(len(approaches)), 2):\n",
        "        approach1, approach2 = approaches[i], approaches[j]\n",
        "        \n",
        "        # Get aligned data\n",
        "        questions = set(results_df[results_df['approach'] == approach1]['question']) & \\\n",
        "                    set(results_df[results_df['approach'] == approach2]['question'])\n",
        "        \n",
        "        aligned_data1 = []\n",
        "        aligned_data2 = []\n",
        "        \n",
        "        for q in questions:\n",
        "            val1 = results_df[(results_df['approach'] == approach1) & \n",
        "                             (results_df['question'] == q)]['answer_correctness'].values\n",
        "            val2 = results_df[(results_df['approach'] == approach2) & \n",
        "                             (results_df['question'] == q)]['answer_correctness'].values\n",
        "            \n",
        "            if len(val1) > 0 and len(val2) > 0:\n",
        "                aligned_data1.append(val1[0])\n",
        "                aligned_data2.append(val2[0])\n",
        "        \n",
        "        if len(aligned_data1) > 5:\n",
        "            stat, p = wilcoxon(aligned_data1, aligned_data2)\n",
        "            \n",
        "            if p < 0.001:\n",
        "                sig_matrix.loc[approach1, approach2] = \"***\"\n",
        "                sig_matrix.loc[approach2, approach1] = \"***\"\n",
        "            elif p < 0.01:\n",
        "                sig_matrix.loc[approach1, approach2] = \"**\"\n",
        "                sig_matrix.loc[approach2, approach1] = \"**\"\n",
        "            elif p < 0.05:\n",
        "                sig_matrix.loc[approach1, approach2] = \"*\"\n",
        "                sig_matrix.loc[approach2, approach1] = \"*\"\n",
        "    \n",
        "    display(sig_matrix)\n",
        "    print(\"\\n* p < 0.05, ** p < 0.01, *** p < 0.001\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Export Tables for Academic Paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export tables for LaTeX\n",
        "if results_df is not None:\n",
        "    print(\"üìÑ EXPORTING TABLES FOR LATEX\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. Main results table\n",
        "    main_results = results_df.groupby('approach')[\n",
        "        ['answer_correctness', 'answer_relevancy', 'faithfulness', 'latency_ms']\n",
        "    ].agg(['mean', 'std']).round(3)\n",
        "    \n",
        "    # Format column names for LaTeX\n",
        "    main_results.columns = [f'{metric}_{stat}' for metric, stat in main_results.columns]\n",
        "    \n",
        "    # Export to LaTeX\n",
        "    latex_table = main_results.to_latex(\n",
        "        caption=\"Performance comparison of different RAG approaches\",\n",
        "        label=\"tab:main_results\",\n",
        "        column_format=\"l\" + \"r\" * len(main_results.columns),\n",
        "        float_format=\"%.3f\"\n",
        "    )\n",
        "    \n",
        "    # Save to file\n",
        "    with open('main_results_table.tex', 'w') as f:\n",
        "        f.write(latex_table)\n",
        "    \n",
        "    print(\"‚úÖ Main results table saved to: main_results_table.tex\")\n",
        "    \n",
        "    # 2. Performance by question type\n",
        "    type_performance = results_df.groupby(['approach', 'question_type'])['answer_correctness'].mean().unstack()\n",
        "    type_performance = type_performance.round(3)\n",
        "    \n",
        "    latex_table_type = type_performance.to_latex(\n",
        "        caption=\"Performance comparison by question type\",\n",
        "        label=\"tab:performance_by_type\",\n",
        "        float_format=\"%.3f\"\n",
        "    )\n",
        "    \n",
        "    with open('performance_by_type_table.tex', 'w') as f:\n",
        "        f.write(latex_table_type)\n",
        "    \n",
        "    print(\"‚úÖ Performance by type table saved to: performance_by_type_table.tex\")\n",
        "    \n",
        "    # 3. Latency breakdown table\n",
        "    latency_breakdown = results_df.groupby('approach')[\n",
        "        ['retrieval_time_ms', 'generation_time_ms', 'latency_ms']\n",
        "    ].mean().round(1)\n",
        "    \n",
        "    latex_table_latency = latency_breakdown.to_latex(\n",
        "        caption=\"Latency breakdown by RAG approach (in milliseconds)\",\n",
        "        label=\"tab:latency_breakdown\",\n",
        "        float_format=\"%.1f\"\n",
        "    )\n",
        "    \n",
        "    with open('latency_breakdown_table.tex', 'w') as f:\n",
        "        f.write(latex_table_latency)\n",
        "    \n",
        "    print(\"‚úÖ Latency breakdown table saved to: latency_breakdown_table.tex\")\n",
        "    \n",
        "    # 4. Create a summary for the paper\n",
        "    print(\"\\nüìù SUMMARY FOR PAPER\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    best_quality = results_df.groupby('approach')['answer_correctness'].mean().idxmax()\n",
        "    best_quality_score = results_df.groupby('approach')['answer_correctness'].mean().max()\n",
        "    \n",
        "    fastest = results_df.groupby('approach')['latency_ms'].mean().idxmin()\n",
        "    fastest_time = results_df.groupby('approach')['latency_ms'].mean().min()\n",
        "    \n",
        "    # Find best trade-off (high quality, reasonable speed)\n",
        "    normalized_quality = results_df.groupby('approach')['answer_correctness'].mean() / \\\n",
        "                        results_df.groupby('approach')['answer_correctness'].mean().max()\n",
        "    normalized_speed = 1 - (results_df.groupby('approach')['latency_ms'].mean() / \\\n",
        "                           results_df.groupby('approach')['latency_ms'].mean().max())\n",
        "    \n",
        "    trade_off_score = (normalized_quality + normalized_speed) / 2\n",
        "    best_tradeoff = trade_off_score.idxmax()\n",
        "    \n",
        "    print(f\"üèÜ Best Quality: {best_quality} (score: {best_quality_score:.3f})\")\n",
        "    print(f\"‚ö° Fastest: {fastest} (latency: {fastest_time:.1f}ms)\")\n",
        "    print(f\"‚öñÔ∏è  Best Trade-off: {best_tradeoff} (combined score: {trade_off_score[best_tradeoff]:.3f})\")\n",
        "    \n",
        "    # Calculate improvements\n",
        "    baseline_score = results_df[results_df['approach'] == 'baseline']['answer_correctness'].mean()\n",
        "    hybrid_score = results_df[results_df['approach'] == 'hybrid_rag']['answer_correctness'].mean()\n",
        "    improvement = ((hybrid_score - baseline_score) / baseline_score) * 100\n",
        "    \n",
        "    print(f\"\\nüìà Hybrid RAG shows {improvement:.1f}% improvement over baseline\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
