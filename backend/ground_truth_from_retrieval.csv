question,ground_truth,source_contexts
What skills are needed for a Data Engineer position?,"Based on the provided job descriptions, the skills needed for a Data Engineer position encompass a range of technical proficiencies and soft skills.  A strong foundation in programming languages like Python (or Go, Java, C++), SQL, Java, or Scala is consistently mentioned.  Experience with big data technologies is frequently cited, including Spark, Hadoop, Kafka, and Beam. Knowledge of ETL/ELT tools such as Airflow, dbt, Nifi, and Talend is also important, as is familiarity with cloud platforms like GCP, Azure, and AWS.  Database experience is also needed, including Microsoft SQL Server, Postgresql, HDFS, Trino, Iceberg or Delta Lake, Snowflake, Redshift, BigQuery, or Synapse.

Beyond technical skills, the job descriptions emphasize the importance of teamwork, communication, problem-solving, and analytical abilities. Experience with data warehousing concepts (Data Lake, Data Warehouse, Lakehouse), DevOps principles, CI/CD, data governance, and data security practices are also beneficial. Experience with data visualization tools like Power BI, Looker, Tableau, or QuickSight is also helpful. Some roles value experience in specific industries like finance or retail, while others prioritize familiarity with specific platforms like Microsoft Fabric, Azure Analytics, or Databricks. Finally, some positions require proficiency in English and the ability to communicate technical solutions to non-technical users.","['Title: Data Engineer\nJob level: \nWorking type: None\nCompany: Công Ty Cổ Phần Chứng Khoán KAFI\n\nQualifications & Skills:\n•   Bachelor/Master Degree in Applied Mathematics, Quantitative and Computational Finance, Computer Science or equivalent domain.\n•   3+ years of experience in Data Engineering or similar role.\n•   Solid technical skill, proficient in programming languages such as Python (or Go, Java, C++) and are willing to learn new technologies.\n•   Experience with big data technologies (Spark, Hadoop, Beam, Kafka) and distributed computing.\n•   Knowledge of ETL tools (Airflow, dbt, Talend, etc.), DevOps and CI/CD for data pipelines.\n•   Hands-on experience with cloud platforms (GCP, Azure, AWS) is a plus.\n•   Previous experience in a business or industry-specific setting (e.g., finance, retail, healthcare) is a plus.\n•   Strong teamwork skills, honesty, dignity, and responsibility.\n•   Precision, passion, open-mindedness, and quick self-learning skills in big data, as well as enthusiasm for solving business problems.\n\nResponsibilities:\n• Engage in the full lifecycle of data platform development, including building and maintaining data warehouses and data lakes, as well as designing, implementing, and optimizing scalable ETL processes (batch/streaming) to support analytics, machine learning, and business intelligence initiatives.\n• Collaborate with development team to integrate data from various sources, including from core system database, APIs and streaming platform.\n• Contribute to building the data infrastructure for a Quant Trading Platform, including real-time market data ingestion, processing, and management to support scalable and efficient algorithmic trading strategies.\n• POC and adopting new technologies to improve data platform management in large-scale high throughput.\n• Ensure data integrity, quality, and governance by implementing best practices in data validation and monitoring.\n• Ensure compliance with security, privacy, and regulatory standards for data handling.\n• Reporting and other tasks relevant to data engineer domain knowledge.', ""Title: Kỹ Sư Dữ Liệu (Data Engineer)\nJob level: \nWorking type: None\nCompany: Công Ty Cổ Phần Chứng Khoán MB\n\nQualifications & Skills:\nGeneral requirements:\n1. Minimum\n3-5 years of experience\nIn the Data Engineer position, including at least 1-2 years in the role of Senior/Lead.\n2. Proficient\nBuilding Pipeline ELT/ETL\nBy Nifi, Kafka, Spark, DBT, Airflow, or equivalent.\n3. Experience working with\nMicrosoft SQL Server, Postgresql, HDFS, Trino, Iceberg or Delta Lake\n.\n4. There is a solid programming platform with\nPython or Java\n, understand APIS, process parallel data, and automate the process.\n5. Understand the model\nData Lake, Data Warehouse and Lakehouse\n.\n6. Experience in handling large amounts of data (Big Data), optimizing system performance.\n7.\nsecurities, banking, finance\n, knowledgeable about market data, trading data, or financial reporting systems.\n8. Skills to analyze and solve good problems, systematic thinking.\n9. The ability to work independently and coordinate well with the team\nPrioritize:\n1. Experience in deploying the system of data warning and testing system.\n2. There is knowledge about data security and access.\n3.\nInterest:\n• Working time 5 days/week (from Monday to Friday).\n• Opportunities for promotion, fair career development.\n• Attractive income, competitiveness, performance.\n• Enjoy social insurance, health insurance, unemployment insurance, health insurance in accordance with the company's regulations.\n• Participate in the company's training and training programs.\n• Participate in the project of large -scale digital conversion, modernizing the entire data platform of the company.\n• opportunities for professional development with new technologies in the field of data\n• Professional, friendly and dynamic working environment.\n• Salary: Agreement\nRequired dossier:\n• Candidate information according to MBS form.\nDeadline for receiving documents:\n• Rolling interview as soon as you receive the application until you are recruiting the appropriate candidate\nThe required dossier when recruited:\n(1) Candidate information according to the MBS form (download at the MBS website).\n(2) Curriculum vitae certified by local authorities in the last 6 months.\n(3) Copy of diploma and related certificates\n(4) With the original securities practice certificate (if any)\n(5) Copy of ID card/Passport/Certified ID card\n(6) Health certificate certified by the hospital in the last 6 months\n\nResponsibilities:\n1. Design and deployment\nData Lake architecture, data warehouse on-prem\nUsing technologies such as Microsoft SQL Server, Hadoop, Iceberg/Delta Lake, PostgreSQL.\n2. Building the process\nElt Pipelines\nWith Apache Nifi, Apache Kafka, Apache Spark, DBT.\n3. Deploying solutions\nReal-time streaming and batch processing\nServing data reporting and analysis systems.\n4. Building Ingest process and processing data from many different sources (trading database, 3rd party API, logs ...).\n5. Ensure quality, reliability and security of data.\n6. Working closely with the data analyst groups, and the sales departments to understand the data needs.\n7. Guide, train and support team members.\n8. Join the system upgrade, optimize data query performance and expand architecture"", 'Title: Data Engineer\nJob level: mid-senior level\nWorking type: fulltime\nCompany: L4 Studio - Software Development Company\n\nQualifications & Skills:\nBachelor s degree in Computer Science, Information Technology, or a related field. 5-8 years of experience in data engineering, database architecture, and data management. Strong problem-solving, debugging, and analytical skills. Ability to work both independently and in a team environment. Excellent communication skills to convey complex ideas to team members and clients. Strong SQL skills and understanding of relational database concepts. Experience integrating structured and unstructured data from multiple sources in batch and streaming modes. Proficiency in cloud computing platforms: AWS, GCP, or Azure. Hands-on experience with ETL tools or cloud data services such as Azure Data Factory, dbt, AWS Glue, or Matillion. Familiarity with modern data warehousing solutions like Snowflake, Redshift, BigQuery, or Synapse. Experience with data visualization tools like Power BI, Looker, Tableau, or QuickSight. Knowledge of Docker and Kubernetes for containerized deployments. Ability to debug and optimize existing data infrastructure and processes. Proficient in English. Nice to have Experience with high-throughput, large-scale data systems. Relevant certifications in cloud, data engineering, or data visualization. Proficiency in at least one programming language Python, Java, or Scala . Exposure to machine learning, AI, and LLMs with practical implementations. Familiarity with legacy data systems Hadoop, Informatica .\n\nResponsibilities:\nAs a Data Engineer, you will play a pivotal role in designing and implementing modern data platforms to support data-driven decision-making. This is a hands-on role that requires expertise in building scalable, efficient, and high-performing data architectures. Beyond project execution, you will contribute to growing the company s consulting practice, including recruiting efforts, technical collateral creation, and staying at the forefront of technology trends through training and certifications. You will also be responsible for building long-term strategic relationships with clients while participating in all aspects of project delivery. Lead discovery sessions with clients to understand their business needs, data requirements, and challenges. Design and develop data architectures, ensuring scalability, security, and efficiency. Build data pipelines to collect, process, and analyze structured and unstructured data from multiple sources. Implement data validation and testing processes to ensure accuracy and efficiency. Automate data collection, processing, and reporting to improve efficiency and reduce manual effort. Create high-quality documentation for problem statements, requirements, solutions, and designs. Support pre-sales activities, including whiteboarding sessions, solution architecture design, and proposal development. Develop reusable and repeatable collateral for use across the practice. Obtain and maintain certifications in relevant cloud and data technologies. Collaborate with the marketing team to produce technical content promoting the company s expertise in data engineering.', ""Title: Data Engineer\nJob level: \nWorking type: None\nCompany: Digital Intellect\n\nQualifications & Skills:\nQualification and Experience:\n•\tBachelor’s degree in computer science, Information Technology, Software Engineering, or 5 years related experience and certifications.\n•\t5+ years of experience in data engineering or SQL development, with hands-on experience building data warehouses, data pipelines or managing data systems.\n•\t2+ years of experience with Microsoft Fabric, Azure Analytics products or Databricks.\n•\tHighly skilled in SQL and Python for data manipulation and transformation.\n•\tImplementation experience of data warehousing or data platform modelling techniques, including Kimball, star schema design, and Medallion architecture.\n•\tExperienced with DevOps principles and practices, including Fabric deployment and data CI/CD pipelines.\n•\tKnowledge of data governance and data security practises including familiarity of Purview or Unity Catalog.\n•\tExperience with ERP systems Dynamics 365, AX and Business Central is beneficial.\n•\tExperience with Power BI and DAX is beneficial.\n•\tAzure Data Engineer Associate certification is beneficial.\nPersonal Attributes and Skills:\n•\tProficient English to effectively communicate with international clients.\n•\tExcellent communication skills, with the ability to explain technical solutions and challenges to non-technical users.\n•\tSelf‐starter with the ability to organise yourself and delivery results.\n•\tAbility to work in a fast-paced dynamic environment and work on multiple tasks simultaneously with strong task management skills.\n•\tAbility to work effectively in a team and collaborate with clients and colleagues.\n•\tStrong analytical and problem-solving skills with the ability to ask for help when needed.\nDigital Intellect Offers You:\n•\tChallenging Opportunities: Projects that push you beyond your comfort zone, promoting professional growth.\n•\tSkill Development & Certifications: Support to enhance your technical expertise and earn certifications.\n•\tDiverse Project Exposure: A variety of projects that expand your commercial and business acumen across multiple industries.\n•\tCareer Growth: An environment designed for continuous learning and development, to become a master data engineer.\n•\tMentorship & Guidance: Benefit from working closely with experienced data leaders who support your career journey.\nBenefits:\n•\tAdditional leave days offered.\n•\tAnnual Tet Bonus.\n•\tCompany-sponsored professional certification and training.\n•\tAll work equipment supplied\nDigital Intellect Culture\nAt our company, we operate like a high-performance sports team. We believe in fostering a results-driven, dynamic environment where every individual is expected to perform at their best, collaborate effectively, and push boundaries to achieve excellence. Just like an elite team, we celebrate individual strengths while working towards a common goal. Success is earned through merit, accountability, and continuous improvement, and we thrive on the challenge of delivering high value to our clients. If you're ready to be part of a driven, focused, and high-performing team, you'll find a home at Digital Intellect.\n\nResponsibilities:\nCompany Overview:\nJoin a boutique Data, Analytics, and AI consulting firm with offices in Sydney and Hanoi. We specialise in delivering tailored, high-quality data solutions across sectors such as manufacturing, software, wholesale, and retail. Leveraging Azure’s suite of services, we help clients harness the power of data to drive strategic decision-making and business transformation.\nRole Overview:\nAs a Data Engineer, you will collaborate closely with the client’s Data Lead to design, develop, and maintain data architectures that enhance their data platforms. You will focus on building robust data pipelines, ensuring data quality, and optimising data systems for analytics and reporting. This role is ideal for someone passionate about solving complex data challenges in a dynamic and collaborative environment.\nResponsibilities:\n•\tData Pipeline Development: Build, test, and maintain data pipelines for real-time and batched data integration, automating processes such as data collection, cleaning, and transformation.\n•\tData Platform Management: Administer and maintain data platforms, ensuring smooth data flow, monitoring system performance, and managing user access and permissions.\n•\tData Quality and Operations: Ensure data quality through validation, cleansing, and transformation, while implementing optimisation techniques like partitioning and indexing.\n•\tProject Support: Assist in executing data integration and migration projects, collaborating with teams on technical requirements and project documentation.\n•\tAnalytics and Troubleshooting: Provide technical support for data models and queries, support user training, and optimise analytics performance.\n•\tGovernance and Compliance: Implement data governance policies, ensuring adherence to privacy, security, and regulatory standards, while maintaining technical documentation."", 'Title: Data Engineer\nJob level: entry level\nWorking type: fulltime\nCompany: Yes4All\n\nQualifications & Skills:\nBachelor s degree in Computer Science, Information Technology, Engineering, or related fields. Minimum of 1vyears in a Data engineering role. Proficient in Python, Java, or Scala. Strong SQL skills and experience with various databases PostgreSQL, MySQL . Expertise in building and optimizing data pipelines and architectures. Knowledge of big data tools Hadoop, Spark, Kafka . Familiar with workflow management tools Azkaban, Luigi, Airflow . Advanced skills in data crawling and scraping using tools like Selenium and BeautifulSoup, capable of overcoming challenges like OTP CAPTCHA. Must have experience running large-scale data crawlers or web scraping operations. Expert in data parsing, with a strong grasp of regular expressions, HTML, CSS, DOM, JavaScript, and XPath. Exceptional ability to analyze and synthesize large datasets with high accuracy and attention to detail. Strong communication skills, able to effectively articulate complex data issues to a diverse stakeholder audience. Working with Trino and MinIO is a plus. Familiarity with data governance and security frameworks. Knowledge of DevOps principles and practices. Experience with AWS cloud services EC2, EMR, RDS, Redshift .\n\nResponsibilities:\nDevelop and maintain high-performance data pipelines, ensuring efficient data ingestion, processing, and analysis across large datasets. Oversee and enhance database performance, catering to both structured and unstructured data, to ensure high reliability and efficiency. Engineer and refine ETL processes for effective data transformation and loading into our data warehousing systems. Continuously monitor and fine-tune system performance to optimize data handling and usage. Collaborate extensively with data scientists, analysts, and cross-functional teams to fulfill their data needs and facilitate seamless data access Tool Integration: Seamlessly integrate new data management technologies and software tools into existing frameworks. Uphold stringent data governance and security standards, ensuring compliance with relevant data privacy regulations. Document key data processes, models, and flows to enhance team capability and data system utility.']"
Can you recommend Python courses for beginners?,"Based on the provided documents, I can recommend two Python courses for beginners: ""Python Basics"" and ""Practical Python: Start Your Programming Journey"".

""Python Basics"" introduces the fundamentals of Python 3, covering conditional execution, iteration, strings, and lists. It will also teach you how to use an on-screen Turtle to draw pictures and how to debug code using reference diagrams. This course is suitable for newcomers to Python or those seeking a refresher. The course covers Chapters 1-9 of the textbook ""Fundamentals of Python Programming.""

""Practical Python: Start Your Programming Journey"" is designed for individuals with no prior programming experience. This course focuses on writing code that takes user input, manipulates data types, and produces results. You'll learn to implement conditionals, random behavior, and loops. By the end of the course, you will be able to develop a text adventure game, a calculator, and generate poems. The course emphasizes planning programming projects and debugging.","['Course name: Python Basics\n    Skills: Computer Graphics, Debugging, Computer Programming, Programming Principles, Scripting Languages, Python Programming, Data Structures, Pseudocode\n    What you will learn: \n    \n    Description: This course introduces the basics of Python 3, including conditional execution and iteration as control structures, and strings and lists as data structures. You\'ll program an on-screen Turtle to draw pretty pictures. You\'ll also learn to draw reference diagrams as a way to reason about program executions, which will help to build up your debugging skills. The course has no prerequisites. It will cover Chapters 1-9 of the textbook ""Fundamentals of Python Programming,"" which is the accompanying text (optional and free) for this course.The course is for you if you\'re a newcomer to Python programming, if you need a refresher on Python basics, or if you may have had some exposure to Python programming but want a more in-depth exposition and vocabulary for describing and reasoning about programs.\n\nThis is the first of five courses in the Python 3 Programming Specialization.', 'Course name: Practical Python: Start Your Programming Journey\n    Skills: Program Development, Pseudocode, Interactive Design, Algorithms, Python Programming, Debugging, Computer Programming, Game Design\n    What you will learn: \n    Read and write code that takes user input and manipulates different kinds of data types (strings, integers, doubles and booleans) and prints results.\nRead and write code that makes decisions (conditionals), can do random behavior, and involves repetition (while loop).\nExplain that programming often involves similar strategies to solve problems and provide an example.\nPlan an outline for a program, translate that plan to code, step through code to check variable values, and recognize parts of an error message.\n    Description: Have you ever wanted to learn programming, where you get to write programs tailored to your interests? How about learning to code while also learning how to design, plan, and implement your projects? If yes, welcome to ""Practical Python: Starting Your Programming Journey!"" In this course, we teach you the beginnings of Python programming while assuming you are starting with no experience. By the end of this course, you will be able to write your own text adventure game, create a personalized calculator, write a poem, and so much more! This course will also introduce you to a process for planning out your programming projects and ideas on how to fix your code when it is not doing what you want.And we will teach you all of this through open-ended assignments that let you decide how to show us what you have learned! Most of our coding assignments have a small set of checks on your code, but otherwise, you get to decide what it does! Want to write code that generates a haiku? You can! Want to write code that generates song lyrics instead? You will get to do that by just week two of the course!']"
What are the responsibilities of a Software Engineer?,"Based on the provided documents, the responsibilities of a Software Engineer vary depending on the company and role. Common responsibilities include writing software programs from design specifications, implementing features, debugging code, and contributing to system architecture and design decisions. Software Engineers are often involved in requirement analysis, system design (creating design documents), coding in programming languages, reverse engineering, and evaluation.

Specific responsibilities can include developing and maintaining cloud service-related software or windows application software, developing and testing software systems, creating graphic screen layouts for human interfaces, commissioning automation systems, and supporting service activities. They also implement task lists, estimate delivery assignments, report status and issues, and ensure proper documentation. Some roles also require optimizing code performance and fixing issues, as well as collaborating with cross-disciplinary teams to design and bring innovative products to market.","['Title: Software Engineer (Java, Python, C++, C#, Node.js)\nJob level: Engineer\nWorking type: None\nCompany: FUJIFILM Business Innovation Việt Nam\n\nQualifications & Skills:\nAt least 3 years of proven work experience as a Software Engineer is required. Development experience in Java or C .NET or Vue.js as a Software Engineer is required. Ability to develop software using Python, C++, Node.js or other similar programming languages is desirable. Experience with Web Application and JavaScript/CSS frameworks is desirable. Experience of using Amazon Web Service AWS is desirable. Knowledge of TLS communication and certificates PKI is desirable Language Proficiency: English intermediate - spoken, listening, and written. Ability to actively communicate and collaborate with colleagues and supervisors. Having the motivation to work for a long period 10 years or more , as a founding member of the company. Bachelor s degree in Computer Science, or a related field, or equivalent experience Relevant technology certifications\n\nResponsibilities:\nThe responsibilities of a Software Engineer include requirement analysis, system design, detailed design creating design documents , coding in programming languages, debugging the created code, reverse engineering and evaluation. You will be developing and maintaining cloud service-related software or windows application software that operates MFP Multi-function printer data handling or IT devices e.g. PC, Server, Mobile, ... management as a cloud service platform. Debugging and evaluation for MFP related cloud service or software will be conducted using either actual devices or simulators. The ultimate role of a Software Engineer is to build high-quality, correctly functioning software in compliance with coding standards.', 'Title: Software Engineer\nJob level: mid-senior level\nWorking type: fulltime\nCompany: AVT Consulting and Technology\n\nQualifications & Skills:\nBachelor s or Master s Degree in Computer Science, Computer Engineering, or related field, or equivalent practical experience. Experience in software development. Ability to work without supervision. Sense of responsibility, team spirit and resilience. 3 years of experience working with infrastructure, software, and/or platforms, with 2 years of experience with cloud services. 2 years of experience building APIs and services using REST, SOAP, JSON, RPC, etc. 1 years of experience in a technical leadership role leading engineer teams in projects and setting technical direction. 1 years of experience working in a complex, matrixed organization involving cross-functional, and/or cross-business projects. Ability to effectively articulate technical challenges and solutions. Adept at handling ambiguous or undefined problems as well as ability to think abstractly.\n\nResponsibilities:\nimplementing features, writing relevant documents, optimizing code performance, and fixing issues. Collaborate with experienced cross-disciplinary experts to conceive, design, and bring innovative products and services to market. Analyze the needs of the user then design and code the right solutions. Recommend, design and code improvements. Work in an agile environment to deliver high-quality software. Ensure proper documentation.', 'Title: Software engineer\nJob level: Engineer\nWorking type: fulltime\nCompany: Damen\n\nQualifications & Skills:\nYou have a bachelor s degree, preferably in Web development With 2-3 years of relevant work experience Knowledge of one or more of these programming languages Knowledge and interest in automation products are a plus Excellent oral and written communication skills in English.\n\nResponsibilities:\nShip AMCS Completion development Developing and testing the software systems on board Creation of graphic screen layout and handling for human interfaces Keep communications discussions with Workboats Technology team to complete develop HMI software Responsible for commissioning of the automation systems delivered, such as conducting start-up according to procedure, performing required tests, peforming acceptance testing with presence of Customer approval societies Support complete the relevant works relating to Commissioning Trials Support to update some kinds of software on board Support Service activities when required.', 'Title: BackEnd Developer\nJob level: mid-senior level\nWorking type: fulltime\nCompany: METUB\n\nDescription: **Responsibility** \n\n\n\n* The Software Engineer is responsible to write software programs from design specifications\n* Determining requirements and providing project estimates and timelines and high level architecture design\n* Implement task lists, estimate deliver assignments as functional specifications, quality standards and project schedules.\n* Reports status and issues\n* Documents detailed designs\n* Contributes to system architecture and design decisions.\n* Reviewing code\n* Perform other tasks assigned by the Product Owner, Project Manager \\& Technical Manager to figure out user’s needs, problems, expectations to contribute ideas \\& solution as well as to resolve them\n\n\n\n  \n\n\n\n\n\n**Required:** \n\n\n\n\n**Must have** \n\n\n\n* Good knowledge of OOP, SOLID principles\n* 3\\+ years experienced in NodeJS , Typescript is a big plus\n* 2\\+ year experienced Postgresql / MongoDB\n* Creative thinking and good at problem solvingStrong knowledge of web service development (RESTful)\n* Excellent communication, motivational, and interpersonal skills.\n\n\n\n**Nice to have** \n\n\n\n* Having experiences mentoring and leading teams\n* Knowledge of cloud services – especially Google Cloud\n* Good knowledge of Docker Engine and K8S, cloud deployment, security\n\nQualifications & Skills:\nMust have Good knowledge of OOP, SOLID principles 3+ years experienced in NodeJS , Typescript is a big plus 2+ year experienced Postgresql / MongoDB Creative thinking and good at problem solvingStrong k\n\nResponsibilities:\nThe Software Engineer is responsible to write software programs from design specifications Determining requirements and providing project estimates and timelines and high level architecture design Implement task lists, estimate deliver assignments as functional specifications, quality standards and project schedules. Reports status and issues Documents detailed designs Contributes to system architecture and design decisions. Reviewing code Perform other tasks assigned by the Product Owner, Project Manager Technical Manager to figure out user s needs, problems, expectations to contribute ideas solution as well as to resolve them', 'Title: Software engineer\nJob level: entry level\nWorking type: fulltime\nCompany: Damen\n\nQualifications & Skills:\nYou have a bachelor s degree, preferably in Web development With 2-3 years of relevant work experience Knowledge of one or more of these programming languages Knowledge and interest in automation products are a plus Excellent oral and written communication skills in English.\n\nResponsibilities:\nShip AMCS Completion development Developing and testing the software systems on board Creation of graphic screen layout and handling for human interfaces Keep communications discussions with Workboats Technology team to complete develop HMI software Responsible for commissioning of the automation systems delivered, such as conducting start-up according to procedure, performing required tests, peforming acceptance testing with presence of Customer approval societies Support complete the relevant works relating to Commissioning Trials Support to update some kinds of software on board Support Service activities when required.']"
