{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c820803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import InferenceClient\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(r\"C:\\Users\\leduc\\OneDrive\\Desktop\\NLP\\Grab-project\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1522a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40abeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "You are an expert resume and job description parser.\n",
    "\n",
    "Given the following input text (which may be a resume or job description), extract and structure the relevant information in the following JSON format:\n",
    "\n",
    "{\n",
    "    \"summary\": \"Provide a concise professional summary based on the resume.\",\n",
    "    \"skills\": [\"List of skills\"],\n",
    "    \"basic_info\": {\n",
    "        \"full_name\": \"Invented Full Name\",\n",
    "        \"university\": \"Generated University Name\",\n",
    "        \"education_level\": \"BS\",\n",
    "        \"majors\": [\"List of Majors\", \"GPA: 3.5\"]\n",
    "    },\n",
    "    \"work_experience\": [{\n",
    "        \"job_title\": \"Title\",\n",
    "        \"company\": \"Company Name\",\n",
    "        \"location\": \"Location\",\n",
    "        \"duration\": \"Duration\",\n",
    "        \"job_summary\": \"Job responsibilities and achievements\"\n",
    "    }],\n",
    "    \"project_experience\": [{\n",
    "        \"project_name\": \"Project Name\",\n",
    "        \"project_description\": \"Project details including technologies used\"\n",
    "    }],\n",
    "    \"publications\": [{\n",
    "        \"publications_name\": \"Project Name\",\n",
    "        \"publications_description\": \"Project details including technologies used\"\n",
    "    }],\n",
    "    \"award\": [{\"award_name\": \"Award Name\"}]\n",
    "}\n",
    "\n",
    "### Guidelines:\n",
    "- Summarize the publications and projects description.\n",
    "- If a field is missing or not available, leave it out of the JSON.\n",
    "- Do not hallucinate data not implied by the text.\n",
    "- Return only the JSON object. No explanation or preamble.\n",
    "\n",
    "Input Text:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbbaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cv_jd_extractor(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    client = InferenceClient(\n",
    "        provider=\"novita\",\n",
    "        api_key=hf_token,\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ad11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "  Jackson McCall\n",
    "Engineer\n",
    "mca_lljcks12@gmail.com | 781-235-7144 | Norwell,\n",
    "\n",
    "Profile\n",
    "\n",
    "Experienced Engineer bringing forth 5+ years providing superior technical guidance and successfully executed engineering designs. Adept in overseeing all aspects of project management including design, implementation, verification, and validation of engineering designs.¬†\n",
    "\n",
    "Work Experience\n",
    "\n",
    "11/2016 - 07/2021, Senior Project Engineer, Valley Field Technologies, Boston\n",
    "\n",
    "Effectively guided technical project development and helped to steer and improve program processes.\n",
    "Planned and executed assigned engineering designs with accuracy.\n",
    "Worked with program managers to ensure proper and adequate technical staffing.\n",
    "Executed design and development efforts in adherence to all engineering design standards.¬†\n",
    "Worked to quickly resolve technical conflicts and issues.\n",
    "Developed and maintained project timelines.\n",
    "Monitored and adjusted control systems.\n",
    "\n",
    "07/2014 - 08/2015, Project Manager, Infotech Builders, Boston\n",
    "\n",
    "Worked to leverage business acumen, leadership experience, and technical skills to deliver results.\n",
    "Oversaw all aspects of project budget, planning, design, and installation.¬†\n",
    "Assessed and communicated project status and potential risks across multiple teams.¬†\n",
    "Planned and conducted weekly standing meetings with internal teams and external vendors and contractors.¬†\n",
    "Proposed continuous improvement strategies with the goal of higher deployment rates.¬†\n",
    "\n",
    "Education\n",
    "\n",
    "09/2014 - 05/2016, Master of Science in Systems Engineering, Boston University, Boston\n",
    "\n",
    "09/2009 - 05/2013, Bachelor of Science in Computer Engineering, Northwestern University, Chicago\n",
    "\n",
    "Languages\n",
    "English\n",
    "German\n",
    "Skills\n",
    "PMI Certified Project Management Professional\n",
    "Microsoft Project\n",
    "MS Dynamics AX\n",
    "Advanced Analytical Thinking Skills\n",
    "Hardware Design\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f66fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_resume(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'(\\+?\\d{1,3})?[-.\\s]?(\\(?\\d{3}\\)?|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '', text)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'\\b(Name|Email|Phone|Contact|LinkedIn|Address|Scholar|ORCID)\\b[:\\s]*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e0855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.5775570869445801\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Example resume and JD texts (in practice, you'd load actual texts)\n",
    "resume_text = \"\"\"\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "jd_text = \"\"\"\n",
    "About the job\n",
    "RegenX, a start-up in regenerative farming and AI Agvisory, is looking for a ü§ñ RAG AI Engineer ü§ñ (full time, hybrid work arrangement).\n",
    "\n",
    "\n",
    "About RegenX:\n",
    "\n",
    "üåè We‚Äôre on a mission to help smallholder farmers adapt to climate change by providing state-of-the-art AI-agvisory while partnering with the ecosystem to accelerate transitions to regenerative agriculture.\n",
    "\n",
    "\n",
    "Here is some more specific information about the role.\n",
    "\n",
    "\n",
    "üéØ The main ‚ÄúJob-To-Be-Done‚Äù is:\n",
    "\n",
    "Build a world class Ai-first platform that enables farmers and buyers to track regenerative transitions progress, outcomes and adjust their actions to adapt to climate change\n",
    "\n",
    "\n",
    "üíº Skills and background we are looking for:\n",
    "\n",
    "\n",
    "\n",
    "Proven ability to build, deploy, and optimize multi-modal RAG pipelines (vision, NLP, voice)\n",
    "Experience with vector databases (Qdrant, Weaviate, Pinecone) and RAG orchestration\n",
    "Comfortable designing data architectures for scalable, low-latency inference pipelines.\n",
    "Experience with document parsing and knowledge retrieval, object detection, image preprocessing.\n",
    "Familiar with cloud-native architectures (GCP, AWS, or Azure), particularly using serverless functions, containerization, and event-driven workflows.\n",
    "Strong software engineering fundamentals, including testing, CI/CD, and observability.\n",
    "\n",
    "\n",
    "üî• Preferred Qualifications:\n",
    "\n",
    "A solid understanding of software engineering fundamentals coupled with strong AI background \n",
    "Experience with PostgreSQL, caching strategies (Redis), and API performance optimization.\n",
    "Fluent in Python, Typescript / Javascript, Postgresql, caching solutions, edge computing\n",
    "Familiarity with Supabase, Firebase, or similar backend platforms for fast prototyping\n",
    "Some understanding of cross-platform development constraints (Flutter, React Native, or platform-specific UIs).\n",
    "Experience working with vector DBs and image embeddings.\n",
    "Awareness of data privacy, security, and compliance requirements when dealing with user data and AI systems.\n",
    "\n",
    "\n",
    "üìù General terms and conditions of the job\n",
    "\n",
    "Full time\n",
    "13 months salary, PVI insurance, BHXH based on full month salary, laptop (after probation) and AI tooling, unlimited leaves, hybrid work arrangement\n",
    "What‚Äôs hybrid work? We meet offline twice a week in HCMC (Vietnam), the rest is remote\n",
    "You may have to travel within Vietnam from time to time to directly meet with farmers and / or partners to build farmer and customer centric tech solutions \n",
    "Interview process: A 15-min call to talk about your ambitions ‚Üí Technical interview ‚Üí Culture Fit interview. Our hiring team is committed to including diverse perspectives and backgrounds throughout the interview process\n",
    "Salary range: we‚Äôll discuss based on the assessment during the interview process\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resume_text = clean_resume(resume_text)\n",
    "jd_text = clean_resume(jd_text)\n",
    "\n",
    "resume_embedding = model.encode(resume_text, convert_to_tensor=True)\n",
    "jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "\n",
    "semantic_similarity = util.pytorch_cos_sim(resume_embedding, jd_embedding).item()\n",
    "\n",
    "\n",
    "print(\"Semantic Similarity:\", semantic_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f659569",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_TOKEN\"])\n",
    "ge_model = genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "611a025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_grading(resume_text: str, jd_text: str):\n",
    "    gemini_input = f\"\"\"\n",
    "    You are an expert technical recruiter and career advisor.\n",
    "\n",
    "    Given the following job description and candidate resume, do the following:\n",
    "\n",
    "    1. **Score the overall match from 0 to 1 it is a float number** based on how well the candidate fits the job requirements.\n",
    "    - if the job title or work is not relevent just return very low score.\n",
    "    2. **List missing or weak areas**\n",
    "    3. **Advise on improvements** ‚Äì suggest what the candidate can add to their CV to better match the role.\n",
    "    4. **List keywords that missing in resume**\n",
    "    5. Be concise but clear, and format the response in Markdown for readability.\n",
    "    6. **OUTPUT FORMAT MUST BE JSON:**\n",
    "\n",
    "    Respond only in the exact JSON format shown below. Do not add any text outside the JSON block.\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Job Description:**\n",
    "    {jd_text}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Candidate Resume:**\n",
    "    {resume_text}\n",
    "\n",
    "    \n",
    "    **Sample Output Format**\n",
    "    ```json\n",
    "    {{\n",
    "      \"overall_match\": 0.43,\n",
    "      \"missing_or_weak_areas\": [\n",
    "        \"No exposure to Unity, Unreal, or any game engine tools.\",\n",
    "        \"No specific focus on AI for game development or visual recognition.\",\n",
    "        \"Missing essential programming languages used in games (C#, C++).\",\n",
    "        \"Limited evidence of direct involvement in collaborative game or AI projects.\"\n",
    "      ],\n",
    "      \"advice_on_improvements\": [\n",
    "        \"Start learning Unity or Unreal Engine and build a simple demo.\",\n",
    "        \"Pick up a course or tutorial in C++ or C# and implement a basic game mechanic.\",\n",
    "        \"Look into AI tasks relevant to game development like enemy pathfinding or procedural world generation.\",\n",
    "        \"Engage in team-based AI projects and include your role in collaboration and design.\"\n",
    "      ],\n",
    "      \"skills_missing\": [\n",
    "        \"Unity\",\n",
    "        \"Unreal Engine\",\n",
    "        \"C#\",\n",
    "        \"C++\",\n",
    "        \"Game AI Concepts\",\n",
    "        \"Team Collaboration in Game Projects\"\n",
    "      ]\n",
    "    }}\n",
    "    ```\n",
    "    **Sample Output Format**\n",
    "    {{\n",
    "      \"overall_match\": 0.91,\n",
    "      \"missing_or_weak_areas\": [],\n",
    "      \"advice_on_improvements\": [\n",
    "        \"Continue building advanced AI mechanics such as adaptive enemy behaviors or procedural world systems.\",\n",
    "        \"Consider contributing to open-source game AI libraries or plugins for Unity/Unreal.\",\n",
    "        \"Explore optimization techniques for real-time AI decision-making under hardware constraints.\"\n",
    "      ],\n",
    "      \"skills_missing\": []\n",
    "    }}\n",
    "\n",
    "    {{\n",
    "      \"overall_match\": 0.17,\n",
    "      \"missing_or_weak_areas\": [\n",
    "        \"No formal education in pharmacology, chemistry, or medicine.\",\n",
    "        \"Lacks required certifications or licensure (e.g., PharmD, pharmacy board exams).\",\n",
    "        \"No knowledge of drug interactions, prescriptions, or patient safety protocols.\",\n",
    "        \"No clinical experience or understanding of healthcare systems and regulations.\"\n",
    "      ],\n",
    "      \"advice_on_improvements\": [\n",
    "        \"Enroll in a formal pharmacy program to gain necessary credentials.\",\n",
    "        \"Complete internship or training at a certified pharmacy or hospital.\",\n",
    "        \"Gain understanding of pharmacokinetics, patient care, and ethical practice.\",\n",
    "        \"Prepare for and pass pharmacy licensing exams (e.g., NAPLEX).\"\n",
    "      ],\n",
    "      \"skills_missing\": [\n",
    "        \"Pharmacology\",\n",
    "        \"Prescription Drug Knowledge\",\n",
    "        \"Clinical Experience\",\n",
    "        \"Pharmacy Licensure\",\n",
    "        \"Healthcare Regulations\",\n",
    "        \"Patient Counseling\",\n",
    "        \"Medical Ethics\"\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    res = ge_model.generate_content(gemini_input, \n",
    "        generation_config={\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_k\": 2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_output_tokens\": 1000,\n",
    "        },\n",
    "        \n",
    "    )\n",
    "\n",
    "    grading = json.loads(res.text.replace(\"```\", \"\").replace(\"json\", \"\"))\n",
    "    return grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b791a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_match 0.65\n",
      "missing_or_weak_areas ['The resume does not explicitly state that the candidate is a native-level English speaker from one of the specified countries (USA, UK, Canada, Australia, New Zealand, Ireland, or South Africa).', 'The resume does not explicitly state possession of CELTA/Trinity certification or an MA in TESOL; it only mentions IELTS score.', 'The resume lacks explicit mention of experience teaching adults.', \"The resume does not include a cover letter tailored to Wall Street English Vietnam, explaining the candidate's motivation to work there.\"]\n",
      "advice_on_improvements ['Clearly state native English speaker status and nationality if applicable.', 'Mention any TEFL certifications held, even if not CELTA/Trinity. Highlight any teaching experience, especially with adults.', \"Craft a cover letter emphasizing a passion for teaching, commitment to English education, and reasons for wanting to work at Wall Street English Vietnam. Tailor the cover letter to align with WSE's values (integrity, support, development).\", \"Quantify experience wherever possible. For example, 'Developed NLP models that improved text summarization accuracy by X%.'\", 'Highlight any experience with cultural exchange or cross-cultural communication.']\n",
      "skills_missing ['CELTA', 'Trinity Certification', 'TESOL', 'Experience teaching adults']\n"
     ]
    }
   ],
   "source": [
    "s = llm_grading(resume_text, jd_text)\n",
    "for k, v in s.items():\n",
    "    print(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c9327d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = \"\"\"\n",
    "About FanduelFanDuel Group is the premier mobile gaming company in the United States and Canada. FanDuel Group consists of a portfolio of leading brands across mobile wagering including: America‚Äôs #1 Sportsbook, FanDuel Sportsbook; its leading iGaming platform, FanDuel Casino; the industry‚Äôs unquestioned leader in horse racing and advance-deposit wagering, FanDuel Racing; and its daily fantasy sports product.In addition, FanDuel Group operates FanDuel TV, its broadly distributed linear cable television network and FanDuel TV+, its leading direct-to-consumer OTT platform. FanDuel Group has a presence across all 50 states, Canada, and Puerto Rico.The company is based in New York with US offices in Los Angeles, Atlanta, and Jersey City, as well as global offices in Canada and Scotland. The company‚Äôs affiliates have offices worldwide, including in Ireland, Portugal, Romania, and Australia.FanDuel Group is a subsidiary of Flutter Entertainment, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and traded on the New York Stock Exchange (NYSE: FLUT).THE POSITIONOur roster has an opening with your name on itIn addition to the specific responsibilities outlined above, employees may be required to perform other such duties as assigned by the Company. This ensures operational flexibility and allows the Company to meet evolving business needs.Our competitive edge is enabled by the ability to leverage accurate and timely data for informed decision-making.‚ÄØWe are seeking a Data Engineer to advance FanDuel‚Äôs data capabilities and contribute to the further growth and maturity of our Data Team.As a Data Engineer, you will be a valued member of the Data Support team, maintaining exceptional standards and servicing all the needs of data consumers across the company. Your role will be to ensure the health of our data infrastructure, contribute to the delivery of reliable, timely, and consumable data for all company data consumers. The success of this team will elevate the capabilities of our Data tribe to maximise the value of our data assets and empower employees to innovate.You will have the opportunity to work in an agile environment where you will team up with colleagues having experience in data warehousing, machine learning and more.The successful candidate should have minimum 3-5 years of data engineering experience with good technical and problem-solving skills, a positive and results driven attitude, and a good communicator who can interact with both technical and non-technical people.THE GAME PLANEveryone on our team has a part to playKey Responsibilities:Provide on-call technical assistance to analysts, data consumers, and data engineering teams, responding to inquiries relating to query optimization, reporting anomalies, complex data pipeline issues, failed jobs, and data inconsistencies.Investigate and resolve data quality issues in response to system alerts or notifications from stakeholders, ensuring compliance with internal/external audits and control practices.Design and implement batch and real-time data pipelines to the data warehouse or data lake using data transformation technologies, enhancing monitoring dashboards and support tools, and automating repetitive support tasks.Lead impactful initiatives to enhance data pipelines and other data operations capabilities while mentoring and onboarding new support data engineers.Create and maintain technical support documentation, including FAQs, troubleshooting steps, support runbooks, and a knowledge base of solutions to recurring issues.Drive operational best practices and shape workflows for routine consumer support, operations monitoring, incident management, and problem management.Administer account access to tools across the technology stack in accordance with SoX governance policies.THE STATSWhat we're looking for in our next teammatePreferred skills:Strong problem-solving skills with the ability to get to the root causes of issues and implement effective solutions.A customer-oriented mindset with a focus on delivering high-quality support services and attention to detail.Minimum 3-5 years of SQL query and query performance tuning experience.Experience with programming languages such as Python, Java, or R.Experience with dimensional data modeling, batch data ingestion, and real-time technologies like Kafka or Kinesis.Understanding of data warehousing and ETL/ELT processing.Experience with cloud databases such as AWS Redshift or Databricks, and Spark on Databricks, AWS, or Azure.Exceptional communication and interpersonal skills.Exposure to orchestration and monitoring tools like Airflow, Datadog, Databand, and Monte Carlo.Familiarity with data visualization tools such as Tableau or Looker.Experience owning and coordinating issues across multiple time zones, bringing continuity of coverage to the Data tribe.Desired skills:Experience with data streaming patterns and ETL/ELT tools like Dbt.Exposure to database design, performance analysis, and data quality metrics.Exposure to Continuous Integration/Continuous Delivery tools like BuildKite or Github Actions.Exposure to Data Science, Machine Learning, and AI.Player BenefitsWe treat our team rightWe offer amazing benefits above and beyond the basics. We have an array of health plans to choose from (some as low as $0 per paycheck) that include programs for fertility and family planning, mental health support, and fitness benefits. We offer generous paid time off (PTO & sick leave), annual bonus and long-term incentive opportunities (based on performance), 401k with up to a 5% match, commuter benefits , pet insurance, and more - check out all our benefits here:FanDuel Total Rewards. *Benefits differ across location, role, and level.FanDuel is an equal opportunities employer and we believe, as one of our principles states, ‚ÄúWe are One Team!‚Äù. As such, we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, veteran status, or another other characteristic protected by state, local or federal law. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included.The applicable salary range for this position is $116,000 - $145,000 USD, which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "495df4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_text( resume_text,jd_text):\n",
    "    gemini_input = f\"\"\"\n",
    "    You are an expert skills detector. You need to summarized resume and job description.\n",
    "\n",
    "    Given the following job description and candidate resume, do the following:\n",
    "    1. **Focus on candidate's skills, required skills, responsibility and projects**\n",
    "    2. **Ignore candidate personal information and company information**.\n",
    "    3. **summarized_resume** and **summarized_jd** MUST be a list of technical skills and domain terminologies.\n",
    "    Respond only in the exact JSON format shown below. Do not add any text outside the JSON block.\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Job Description:**\n",
    "    {jd_text}\n",
    "\n",
    "    ---\n",
    "\n",
    "    **Candidate Resume:**\n",
    "    {resume_text}\n",
    "\n",
    "    \n",
    "    **Sample Output Format**\n",
    "    ```json\n",
    "    {{\n",
    "    \"summarized_resume\": [],\n",
    "    \"summarized_jd\": []\n",
    "    }}\n",
    "\n",
    "\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    res = ge_model.generate_content(gemini_input, \n",
    "            generation_config={\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_k\": 3,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_output_tokens\": 1000,\n",
    "            },\n",
    "            \n",
    "        )\n",
    "\n",
    "    return json.loads(res.text.replace(\"```\", \"\").replace(\"json\", \"\")) \n",
    "\n",
    "# s = sum_text(resume_text, jd_text)\n",
    "# for k, v in s.items():\n",
    "#     print(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e014dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_matching(resume_keywords, jd_keywords):\n",
    "    \n",
    "    \n",
    "    matching_keywords = set(resume_keywords) & set(jd_keywords)\n",
    "    \n",
    "    \n",
    "    score = len(matching_keywords) / max(len(set(jd_keywords)), len(set(resume_keywords)))  \n",
    "    return score*2, matching_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "35247e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_scoring(resume_text, jd_text):\n",
    "\n",
    "    resume_text = \"passage: \" + resume_text\n",
    "    jd_text = \"query: \" + jd_text \n",
    "    resume_embedding = model.encode(resume_text, convert_to_tensor=True)\n",
    "    jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "\n",
    "    semantic_similarity = util.pytorch_cos_sim(resume_embedding, jd_embedding).item()\n",
    "\n",
    "    return semantic_similarity\n",
    "\n",
    "def final_score(resume_text, jd_text):\n",
    "    llm_s = llm_grading(resume_text, jd_text)\n",
    "    se_s = semantic_scoring(resume_text, jd_text)\n",
    "    print(llm_s)\n",
    "    print(se_s)\n",
    "    return 0.7*llm_s[\"overall_match\"] + 0.3*se_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9e578d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall_match': 0.65, 'missing_or_weak_areas': ['Limited experience with cloud databases such as AWS Redshift or Databricks, and Spark on Databricks, AWS, or Azure.', 'Exposure to orchestration and monitoring tools like Airflow, Datadog, Databand, and Monte Carlo is not clearly demonstrated.', 'Experience with data streaming patterns and ETL/ELT tools like Dbt is not explicitly mentioned.', 'Experience owning and coordinating issues across multiple time zones is not evident.'], 'advice_on_improvements': ['Highlight any experience with cloud data platforms (AWS, Azure, GCP) and specific services like Redshift, Databricks, or Spark.', 'Include projects or experiences where you used orchestration tools (e.g., Airflow) or monitoring tools (e.g., Datadog).', 'If you have used data streaming technologies (e.g., Kafka, Kinesis) or ETL/ELT tools (e.g., DBT), add them to your resume with specific examples.', 'Elaborate on any experience coordinating or resolving issues across different time zones or teams.'], 'skills_missing': ['AWS Redshift', 'Databricks', 'Spark', 'Airflow', 'Datadog', 'Databand', 'Monte Carlo', 'DBT', 'Kafka', 'Kinesis', 'ETL/ELT', 'Data Streaming']}\n",
      "0.4190078377723694\n",
      "0.5807023513317108\n"
     ]
    }
   ],
   "source": [
    "print(final_score(resume_text, jd_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67191e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
